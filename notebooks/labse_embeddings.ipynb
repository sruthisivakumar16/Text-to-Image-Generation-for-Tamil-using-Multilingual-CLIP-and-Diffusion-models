{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziGkPW9eeV4i",
        "outputId": "f288e0c6-8c72-4ac0-9913-c303df446eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJCwC1AdeX9X",
        "outputId": "1875c054-5348-45c5-8fa0-61138e3c10fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3G0ZxWDefX2",
        "outputId": "608d8e68-a559-429c-ae89-e22063a21194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multilingual-clip\n",
            "  Downloading multilingual_clip-1.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from multilingual-clip) (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers->multilingual-clip) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers->multilingual-clip) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->multilingual-clip) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->multilingual-clip) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->multilingual-clip) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->multilingual-clip) (2024.7.4)\n",
            "Downloading multilingual_clip-1.0.10-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: multilingual-clip\n",
            "Successfully installed multilingual-clip-1.0.10\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-2gcio3gc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-2gcio3gc\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.18.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369492 sha256=1038ac46b7a27fd2fa27772f3c5a4706b3017f11e82acf22451bba62fe7e11ba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6yd5nv_z/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.2.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install multilingual-clip\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QvcB5DAenmL",
        "outputId": "e7fe8d64-191e-4aed-c995-6a09057fc893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of train images: 29000\n",
            "length of train captions: 29000\n",
            "length of val images: 1014\n",
            "length of val captions: 1014\n",
            "length of test images: 1000\n",
            "length of test captions: 1000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.multiprocessing import set_start_method\n",
        "from multilingual_clip import pt_multilingual_clip\n",
        "import transformers\n",
        "import clip\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure using 'spawn' start method for CUDA\n",
        "try:\n",
        "    set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "    pass\n",
        "\n",
        "#dataset directory\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/thesis/MMDravi/'\n",
        "\n",
        "# Define paths to the images and split files\n",
        "image_splits_dir = os.path.join(data_dir, 'image_splits')\n",
        "images_dir = os.path.join(data_dir, 'flickr30k_images/flickr30k_images')\n",
        "\n",
        "#read image split files\n",
        "def read_image_splits(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return [line.strip() for line in file]\n",
        "\n",
        "#load image splits\n",
        "train_images = read_image_splits(os.path.join(image_splits_dir, 'train_images.txt'))\n",
        "val_images = read_image_splits(os.path.join(image_splits_dir, 'val_images.txt'))\n",
        "test_images_2016 = read_image_splits(os.path.join(image_splits_dir, 'test_2016_images.txt'))\n",
        "#the image files in split are not found in the flickr30k images folder. skipping this for now\n",
        "#test_images_2017 = read_image_splits(os.path.join(image_splits_dir, 'test_2017_images.txt'))\n",
        "\n",
        "#load captions\n",
        "def load_captions(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return [line.strip() for line in file]\n",
        "\n",
        "#load paths of caption files\n",
        "train_captions_files = [\n",
        "    os.path.join(data_dir, 'train.lc.norm.tok.1.google.ta'),\n",
        "    os.path.join(data_dir, 'train.lc.norm.tok.2.google.ta'),\n",
        "    os.path.join(data_dir, 'train.lc.norm.tok.3.google.ta')\n",
        "]\n",
        "val_captions_file = os.path.join(data_dir, 'val.lc.norm.tok.google.ta')\n",
        "test_captions_file_2016 = os.path.join(data_dir, 'test_2016_flickr.lc.norm.tok.google.ta')\n",
        "test_captions_file_2017 = os.path.join(data_dir, 'test_2017_flickr.lc.norm.tok.google.ta')\n",
        "\n",
        "#load captions\n",
        "train_captions = []\n",
        "for file in train_captions_files:\n",
        "    train_captions.extend(load_captions(file))\n",
        "val_captions = load_captions(val_captions_file)\n",
        "test_captions_2016 = load_captions(test_captions_file_2016)\n",
        "#test_captions_2017 = load_captions(test_captions_file_2017)\n",
        "\n",
        "#checking the lengths\n",
        "print(\"length of train images:\", len(train_images))\n",
        "print(\"length of train captions:\", len(train_captions))\n",
        "print(\"length of val images:\", len(val_images))\n",
        "print(\"length of val captions:\", len(val_captions))\n",
        "print(\"length of test images:\", len(test_images_2016))\n",
        "print(\"length of test captions:\", len(test_captions_2016))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-OhW6Jzihtz",
        "outputId": "1dc9cdc0-82d4-437a-f5be-e5f1726bc883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "M-CLIP/LABSE-Vit-L-14 text model loaded successfuly\n",
            "Tokenizer loaded successfully\n",
            "Models and tokenizers loaded successfully\n",
            "DataLoader created with 29000 samples\n",
            "DataLoader created with 1014 samples\n",
            "DataLoader created with 1000 samples\n",
            "Data loaders created successfully.\n"
          ]
        }
      ],
      "source": [
        "#dataset class\n",
        "class MMDraviDataset(Dataset):\n",
        "    def __init__(self, image_paths, captions, images_dir, preprocess, tokenizer, text_model, device):\n",
        "        self.image_paths = image_paths\n",
        "        self.captions = captions\n",
        "        self.images_dir = images_dir\n",
        "        self.preprocess = preprocess\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_model = text_model\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        caption = self.captions[idx]\n",
        "\n",
        "        try:\n",
        "            # Process image\n",
        "            image = Image.open(os.path.join(self.images_dir, image_path)).convert(\"RGB\")\n",
        "            image = self.preprocess(image).to(self.device)\n",
        "            return image_path, image, caption\n",
        "        except IOError as e:\n",
        "            print(f\"Error processing {image_path}: {e}\")\n",
        "            return None  # Return None for image if there is an error\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error processing {image_path}: {e}\")\n",
        "            return None  # Return None for image if there is an error\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Filter out None images and log which ones are None\n",
        "    valid_items = []\n",
        "    for item in batch:\n",
        "        if item is not None:\n",
        "            valid_items.append(item)\n",
        "        else:\n",
        "            print(\"Found None item in batch\")\n",
        "\n",
        "    if len(valid_items) == 0:\n",
        "        return [], [], []  # Return empty lists if the entire batch is None\n",
        "\n",
        "    return default_collate(valid_items)\n",
        "\n",
        "#creating data loaders\n",
        "def create_data_loader(image_paths, captions, images_dir, preprocess, tokenizer, text_model, device, batch_size=64, shuffle=True):\n",
        "    dataset = MMDraviDataset(image_paths, captions, images_dir, preprocess, tokenizer, text_model, device)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=custom_collate_fn)\n",
        "    print(f\"DataLoader created with {len(dataset)} samples\")\n",
        "    return data_loader\n",
        "\n",
        "#define the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#load models and tokenizers\n",
        "try:\n",
        "    text_model_name = 'M-CLIP/LABSE-Vit-L-14'\n",
        "    text_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(text_model_name)\n",
        "    print(f\"{text_model_name} text model loaded successfuly\")\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(text_model_name)\n",
        "    print(\"Tokenizer loaded successfully\")\n",
        "    image_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
        "    print(\"Models and tokenizers loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models or tokenizers: {e}\")\n",
        "    raise e\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = create_data_loader(train_images, train_captions, images_dir, preprocess, tokenizer, text_model, device)\n",
        "val_loader = create_data_loader(val_images, val_captions, images_dir, preprocess, tokenizer, text_model, device, shuffle=False)\n",
        "test_loader_2016 = create_data_loader(test_images_2016, test_captions_2016, images_dir, preprocess, tokenizer, text_model, device, shuffle=False)\n",
        "print(\"Data loaders created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGnTXqOguuKs"
      },
      "outputs": [],
      "source": [
        "#collecting and saving embeddings\n",
        "def collect_and_save_embeddings(data_loader, image_model, text_model, tokenizer, device, save_path):\n",
        "    all_embeddings = []\n",
        "    #print(\"Entering loop\") #print statements for debugging\n",
        "\n",
        "    #using tqdm to create a progress bar\n",
        "    for i, (image_paths, images, captions) in enumerate(tqdm(data_loader, desc=\"Processing batches\")):\n",
        "        #print(\"checking for valid images\")\n",
        "        if len(images) == 0:\n",
        "          print(f\"All images in batch {i + 1} are None. Skipping this batch.\")\n",
        "          continue\n",
        "\n",
        "        try:\n",
        "            #print(f\"Processing batch {i + 1}/{len(data_loader)}\")\n",
        "            images = images.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                try:\n",
        "                    image_embeds = image_model.encode_image(images)\n",
        "                    #print(\"Image embeddings extracted\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting image embeddings in batch {i + 1}: {e}\")\n",
        "                    continue  #skip this batch\n",
        "\n",
        "                try:\n",
        "                    text_embeds = text_model.forward(captions, tokenizer)\n",
        "                    #print(\"Text embeddings extracted\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting text embeddings in batch {i + 1}: {e}\")\n",
        "                    continue  #skip this batch\n",
        "\n",
        "            #print(\"Collecting embeddings with identifiers\")\n",
        "            #add embeddings with identifiers\n",
        "            for img_path, img_emb, txt_emb, caption in zip(image_paths, image_embeds, text_embeds, captions):\n",
        "                all_embeddings.append({\n",
        "                    'image_path': img_path,\n",
        "                    'image_embedding': img_emb.cpu(),\n",
        "                    'text_embedding': txt_emb.cpu(),\n",
        "                    'caption': caption\n",
        "                })\n",
        "            #print(\"Collected embeddings with identifiers for batch\", i + 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in processing batch {i + 1}: {e}\")\n",
        "\n",
        "    # Save all embeddings to a single file\n",
        "    try:\n",
        "        torch.save(all_embeddings, save_path)\n",
        "        print(f\"Saved embeddings to {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving embeddings to {save_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMpN6VV82_rE"
      },
      "outputs": [],
      "source": [
        "save_dir = '/content/drive/MyDrive/Colab Notebooks/thesis/embeddings/labse'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2ZY9rO__68W",
        "outputId": "c7ebb23f-dd38-4103-80ec-c5ec3bf20a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(text_model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rt5ERh5239M",
        "outputId": "150b65c9-7b93-428c-886d-f431bceec7ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 454/454 [3:14:45<00:00, 25.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved embeddings to /content/drive/MyDrive/Colab Notebooks/thesis/embeddings/labse/labse_train_embeddings.pth\n"
          ]
        }
      ],
      "source": [
        "collect_and_save_embeddings(train_loader, image_model, text_model, tokenizer, device, os.path.join(save_dir, 'labse_train_embeddings.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJFsBbuo4bSY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "#preview the first few entries\n",
        "def preview_embeddings(embeddings, num_entries=5):\n",
        "    print(f\"Total number of embeddings: {len(embeddings)}\")\n",
        "    for i, entry in enumerate(embeddings[:num_entries]):\n",
        "        print(f\"\\nEntry {i + 1}:\")\n",
        "        print(f\"Image Path: {entry['image_path']}\")\n",
        "        print(f\"Image Embedding: {entry['image_embedding'][:5]}...\")  #print the first few values of the embedding\n",
        "        print(f\"Text Embedding: {entry['text_embedding'][:5]}...\")\n",
        "        print(f\"Caption: {entry['caption']}\")\n",
        "\n",
        "# Path to the saved train embeddings file\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/thesis/embeddings/labse/labse_train_embeddings.pth'\n",
        "\n",
        "#load the embeddings\n",
        "labse_train_embeddings = torch.load(save_path)\n",
        "\n",
        "preview_embeddings(labse_train_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collect_and_save_embeddings(test_loader_2016, image_model, text_model, tokenizer, device, os.path.join(save_dir, 'labse_test_2016_embeddings.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OkcR_GOfVXH",
        "outputId": "9d8f5d9f-1fad-4f8c-8165-01a6d78690cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches: 100%|██████████| 16/16 [11:58<00:00, 44.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings to /content/drive/MyDrive/Colab Notebooks/thesis/embeddings/labse/labse_test_2016_embeddings.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the saved train embeddings file\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/thesis/embeddings/labse/labse_test_2016_embeddings.pth'\n",
        "\n",
        "#load the embeddings\n",
        "labse_test_2016_embeddings = torch.load(save_path)\n",
        "\n",
        "preview_embeddings(labse_test_2016_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMCjqK1JiZNk",
        "outputId": "f5160039-d92b-4f0a-e303-91d853338780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of embeddings: 1000\n",
            "\n",
            "Entry 1:\n",
            "Image Path: 1007129816.jpg\n",
            "Image Embedding: tensor([ 0.5273,  0.0310,  0.7612, -0.5669, -0.3145], dtype=torch.float16)...\n",
            "Text Embedding: tensor([-0.0741,  0.2546, -0.1395, -0.0054,  0.2642])...\n",
            "Caption: ஒரு ஆரஞ்சு தொப்பி ஒரு மனிதன் ஏதாவது நடித்தார்.\n",
            "\n",
            "Entry 2:\n",
            "Image Path: 1009434119.jpg\n",
            "Image Embedding: tensor([ 5.5029e-01,  8.1494e-01,  2.8125e-01, -3.5645e-01,  3.8910e-04],\n",
            "       dtype=torch.float16)...\n",
            "Text Embedding: tensor([ 0.4257,  0.3573,  0.0326, -0.2773, -0.0328])...\n",
            "Caption: ஒரு போஸ்டன் டெரியர் ஒரு வெள்ளை வேலி முன் பசுமையான புல் இயங்கும்.\n",
            "\n",
            "Entry 3:\n",
            "Image Path: 101362133.jpg\n",
            "Image Embedding: tensor([ 0.0176, -0.3918,  0.5522,  0.7544, -0.5361], dtype=torch.float16)...\n",
            "Text Embedding: tensor([-0.0120, -0.4445,  0.1994,  0.0087, -0.0450])...\n",
            "Caption: கரேட் சீருடையில் ஒரு பெண் முன் ஒரு கிக் ஒரு குச்சி உடைத்து.\n",
            "\n",
            "Entry 4:\n",
            "Image Path: 102617084.jpg\n",
            "Image Embedding: tensor([ 0.5654,  0.6143,  0.3904,  0.0358, -0.3350], dtype=torch.float16)...\n",
            "Text Embedding: tensor([-0.1117,  1.0427,  0.1241,  0.8254, -0.3226])...\n",
            "Caption: குளிர்கால ஜாக்கெட்டுகள் மற்றும் தலைக்கவசங்களுடன் அணிந்துகொண்டிருக்கும் ஐந்து பேர் பனிப்பகுதியில் நிற்கிறார்கள், பின்னணியில் பனிமலைகள் உள்ளன.\n",
            "\n",
            "Entry 5:\n",
            "Image Path: 10287332.jpg\n",
            "Image Embedding: tensor([ 0.7017,  0.4949, -0.1907,  1.7715, -0.1583], dtype=torch.float16)...\n",
            "Text Embedding: tensor([-0.6019,  0.1323, -0.3586,  0.4419,  0.4154])...\n",
            "Caption: மக்கள் ஒரு வீட்டின் கூரையை சரிசெய்கின்றனர்.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collect_and_save_embeddings(val_loader, image_model, text_model, tokenizer, device, os.path.join(save_dir, 'labse_val_embeddings.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yhrhtoWhNZE",
        "outputId": "a740ce37-157f-4389-d5e8-42b575272368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches: 100%|██████████| 16/16 [13:54<00:00, 52.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings to /content/drive/MyDrive/Colab Notebooks/thesis/embeddings/labse/labse_val_embeddings.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preview_embeddings(embeddings, num_entries=5):\n",
        "    print(f\"Total number of embeddings: {len(embeddings)}\")\n",
        "    for i, entry in enumerate(embeddings[:num_entries]):\n",
        "        print(f\"\\nEntry {i + 1}:\")\n",
        "        print(f\"Image Path: {entry['image_path']}\")\n",
        "        print(f\"Image Embedding: {entry['image_embedding'][:5]}...\")  #print the first few values of the embedding\n",
        "        print(f\"Text Embedding: {entry['text_embedding'][:5]}...\")\n",
        "        print(f\"Caption: {entry['caption']}\")"
      ],
      "metadata": {
        "id": "ozjKllXRn0hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the saved train embeddings file\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/thesis/embeddings/labse/labse_val_embeddings.pth'\n",
        "\n",
        "#load the embeddings\n",
        "labse_val_embeddings = torch.load(save_path)\n",
        "\n",
        "preview_embeddings(labse_val_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eiTzG-zn19T",
        "outputId": "6b818984-3bd8-4b45-d8d9-8a51947f5e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of embeddings: 1014\n",
            "\n",
            "Entry 1:\n",
            "Image Path: 1018148011.jpg\n",
            "Image Embedding: tensor([ 0.1012,  0.4971, -0.6328,  0.7017,  0.4465], dtype=torch.float16)...\n",
            "Text Embedding: tensor([-0.1718,  0.2804, -0.1001,  0.4348,  0.2444])...\n",
            "Caption: ஆண்கள் ஒரு குழு ஒரு டிரக் மீது பருத்தி ஏற்றும்\n",
            "\n",
            "Entry 2:\n",
            "Image Path: 1029450589.jpg\n",
            "Image Embedding: tensor([ 0.9038,  1.3730,  0.2546,  0.8823, -0.1313], dtype=torch.float16)...\n",
            "Text Embedding: tensor([-0.0818,  0.8853,  0.2320, -0.4320, -0.2134])...\n",
            "Caption: ஒரு படுக்கையில் ஒரு பச்சை அறையில் ஒரு மனிதன் தூங்கி.\n",
            "\n",
            "Entry 3:\n",
            "Image Path: 1029737941.jpg\n",
            "Image Embedding: tensor([ 0.7334, -0.0198, -0.1046,  1.1055, -0.2297], dtype=torch.float16)...\n",
            "Text Embedding: tensor([ 0.3376,  0.0708,  0.0562, -0.4094, -0.0230])...\n",
            "Caption: ஹெட்ஃபோன்களை அணிந்து கொண்டிருக்கும் ஒரு பெண் ஒரு பெண் மற்றும் அபோஸ் தோள்களில் அமர்ந்துள்ளார்.\n",
            "\n",
            "Entry 4:\n",
            "Image Path: 103205630.jpg\n",
            "Image Embedding: tensor([ 0.6084,  0.4980, -0.8193,  0.0561, -0.5044], dtype=torch.float16)...\n",
            "Text Embedding: tensor([-0.1087,  0.0052, -0.4808, -0.4211,  0.1258])...\n",
            "Caption: இரண்டு ஆண்கள் ஒரு நீல ஏரி மீது ஒரு நீல பனி மீன்பிடி குடிசை அமைக்க\n",
            "\n",
            "Entry 5:\n",
            "Image Path: 10350842.jpg\n",
            "Image Embedding: tensor([ 0.9990, -0.1664, -0.4644, -0.9976, -0.3652], dtype=torch.float16)...\n",
            "Text Embedding: tensor([-0.0579, -0.3869, -0.6388, -1.3081, -0.5055])...\n",
            "Caption: ஒரு சிவப்பு வாழ்க்கை ஜாக்கெட்டை அணிந்த ஒரு துணிச்சலான மனிதன் ஒரு சிறிய படகில் உட்கார்ந்திருக்கிறான்.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}